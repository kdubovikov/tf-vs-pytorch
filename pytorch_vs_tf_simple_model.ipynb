{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a companion notebook for my post here. The notebook outlines similarities and differences between Pytorch and Tensorflow. \n",
    "\n",
    "First, let's create a simple SGD approximator for $f(x) = x^{exp}$, where $exp$ is a model parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plain Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "loss = 31486.984375\n",
      "exp = 4.0\n",
      "Iteration 1\n",
      "loss = 14398.49609375\n",
      "exp = 3.64985728263855\n",
      "Iteration 2\n",
      "loss = 9984.84765625\n",
      "exp = 3.4879090785980225\n",
      "Iteration 3\n",
      "loss = 7705.31103515625\n",
      "exp = 3.3744404315948486\n",
      "Iteration 4\n",
      "loss = 6282.35546875\n",
      "exp = 3.2859721183776855\n",
      "Iteration 5\n",
      "loss = 5300.4287109375\n",
      "exp = 3.2130908966064453\n",
      "Iteration 6\n",
      "loss = 4578.4267578125\n",
      "exp = 3.1509552001953125\n",
      "Iteration 7\n",
      "loss = 4023.53076171875\n",
      "exp = 3.0967135429382324\n",
      "Iteration 8\n",
      "loss = 3582.866455078125\n",
      "exp = 3.0485341548919678\n",
      "Iteration 9\n",
      "loss = 3223.950927734375\n",
      "exp = 3.0051655769348145\n",
      "Iteration 10\n",
      "loss = 2925.66259765625\n",
      "exp = 2.965712785720825\n",
      "Iteration 11\n",
      "loss = 2673.640625\n",
      "exp = 2.9295122623443604\n",
      "Iteration 12\n",
      "loss = 2457.76611328125\n",
      "exp = 2.8960580825805664\n",
      "Iteration 13\n",
      "loss = 2270.693359375\n",
      "exp = 2.864955186843872\n",
      "Iteration 14\n",
      "loss = 2106.953857421875\n",
      "exp = 2.835888624191284\n",
      "Iteration 15\n",
      "loss = 1962.39306640625\n",
      "exp = 2.8086037635803223\n",
      "Iteration 16\n",
      "loss = 1833.79248046875\n",
      "exp = 2.782891035079956\n",
      "Iteration 17\n",
      "loss = 1718.62158203125\n",
      "exp = 2.7585763931274414\n",
      "Iteration 18\n",
      "loss = 1614.8603515625\n",
      "exp = 2.735513210296631\n",
      "Iteration 19\n",
      "loss = 1520.87744140625\n",
      "exp = 2.7135770320892334\n",
      "Iteration 20\n",
      "loss = 1435.34033203125\n",
      "exp = 2.6926612854003906\n",
      "Iteration 21\n",
      "loss = 1357.1495361328125\n",
      "exp = 2.6726739406585693\n",
      "Iteration 22\n",
      "loss = 1285.3892822265625\n",
      "exp = 2.6535348892211914\n",
      "Iteration 23\n",
      "loss = 1219.291259765625\n",
      "exp = 2.63517427444458\n",
      "Iteration 24\n",
      "loss = 1158.205078125\n",
      "exp = 2.617530345916748\n",
      "Iteration 25\n",
      "loss = 1101.576904296875\n",
      "exp = 2.600548505783081\n",
      "Iteration 26\n",
      "loss = 1048.931396484375\n",
      "exp = 2.5841801166534424\n",
      "Iteration 27\n",
      "loss = 999.8600463867188\n",
      "exp = 2.5683820247650146\n",
      "Iteration 28\n",
      "loss = 954.0079956054688\n",
      "exp = 2.5531153678894043\n",
      "Iteration 29\n",
      "loss = 911.0660400390625\n",
      "exp = 2.5383450984954834\n",
      "Iteration 30\n",
      "loss = 870.763427734375\n",
      "exp = 2.5240395069122314\n",
      "Iteration 31\n",
      "loss = 832.8624877929688\n",
      "exp = 2.5101699829101562\n",
      "Iteration 32\n",
      "loss = 797.1533813476562\n",
      "exp = 2.4967105388641357\n",
      "Iteration 33\n",
      "loss = 763.4500732421875\n",
      "exp = 2.4836373329162598\n",
      "Iteration 34\n",
      "loss = 731.5864868164062\n",
      "exp = 2.470928430557251\n",
      "Iteration 35\n",
      "loss = 701.4149169921875\n",
      "exp = 2.458563804626465\n",
      "Iteration 36\n",
      "loss = 672.8035888671875\n",
      "exp = 2.4465253353118896\n",
      "Iteration 37\n",
      "loss = 645.6333618164062\n",
      "exp = 2.43479585647583\n",
      "Iteration 38\n",
      "loss = 619.7974243164062\n",
      "exp = 2.4233598709106445\n",
      "Iteration 39\n",
      "loss = 595.19921875\n",
      "exp = 2.412202835083008\n",
      "Iteration 40\n",
      "loss = 571.7513427734375\n",
      "exp = 2.4013113975524902\n",
      "Iteration 41\n",
      "loss = 549.3745727539062\n",
      "exp = 2.3906731605529785\n",
      "Iteration 42\n",
      "loss = 527.9961547851562\n",
      "exp = 2.3802762031555176\n",
      "Iteration 43\n",
      "loss = 507.5509033203125\n",
      "exp = 2.370110034942627\n",
      "Iteration 44\n",
      "loss = 487.9783630371094\n",
      "exp = 2.3601644039154053\n",
      "Iteration 45\n",
      "loss = 469.2232971191406\n",
      "exp = 2.3504297733306885\n",
      "Iteration 46\n",
      "loss = 451.2353820800781\n",
      "exp = 2.34089732170105\n",
      "Iteration 47\n",
      "loss = 433.9684753417969\n",
      "exp = 2.3315589427948\n",
      "Iteration 48\n",
      "loss = 417.37933349609375\n",
      "exp = 2.322406530380249\n",
      "Iteration 49\n",
      "loss = 401.4288635253906\n",
      "exp = 2.3134329319000244\n",
      "Iteration 50\n",
      "loss = 386.0806884765625\n",
      "exp = 2.304631233215332\n",
      "Iteration 51\n",
      "loss = 371.30084228515625\n",
      "exp = 2.295994758605957\n",
      "Iteration 52\n",
      "loss = 357.0584716796875\n",
      "exp = 2.287517547607422\n",
      "Iteration 53\n",
      "loss = 343.3243713378906\n",
      "exp = 2.279193639755249\n",
      "Iteration 54\n",
      "loss = 330.0716552734375\n",
      "exp = 2.271017551422119\n",
      "Iteration 55\n",
      "loss = 317.2752380371094\n",
      "exp = 2.262984037399292\n",
      "Iteration 56\n",
      "loss = 304.9120788574219\n",
      "exp = 2.2550883293151855\n",
      "Iteration 57\n",
      "loss = 292.9603271484375\n",
      "exp = 2.2473256587982178\n",
      "Iteration 58\n",
      "loss = 281.39947509765625\n",
      "exp = 2.2396914958953857\n",
      "Iteration 59\n",
      "loss = 270.2108459472656\n",
      "exp = 2.2321817874908447\n",
      "Iteration 60\n",
      "loss = 259.376220703125\n",
      "exp = 2.224792242050171\n",
      "Iteration 61\n",
      "loss = 248.87936401367188\n",
      "exp = 2.2175192832946777\n",
      "Iteration 62\n",
      "loss = 238.70437622070312\n",
      "exp = 2.2103590965270996\n",
      "Iteration 63\n",
      "loss = 228.83676147460938\n",
      "exp = 2.203308343887329\n",
      "Iteration 64\n",
      "loss = 219.2627410888672\n",
      "exp = 2.196363687515259\n",
      "Iteration 65\n",
      "loss = 209.96939086914062\n",
      "exp = 2.1895220279693604\n",
      "Iteration 66\n",
      "loss = 200.944091796875\n",
      "exp = 2.1827800273895264\n",
      "Iteration 67\n",
      "loss = 192.1757049560547\n",
      "exp = 2.1761350631713867\n",
      "Iteration 68\n",
      "loss = 183.6532440185547\n",
      "exp = 2.169584274291992\n",
      "Iteration 69\n",
      "loss = 175.36647033691406\n",
      "exp = 2.1631250381469727\n",
      "Iteration 70\n",
      "loss = 167.3055877685547\n",
      "exp = 2.156754732131958\n",
      "Iteration 71\n",
      "loss = 159.4615478515625\n",
      "exp = 2.1504709720611572\n",
      "Iteration 72\n",
      "loss = 151.82554626464844\n",
      "exp = 2.1442713737487793\n",
      "Iteration 73\n",
      "loss = 144.38954162597656\n",
      "exp = 2.1381537914276123\n",
      "Iteration 74\n",
      "loss = 137.1457061767578\n",
      "exp = 2.1321160793304443\n",
      "Iteration 75\n",
      "loss = 130.0865936279297\n",
      "exp = 2.1261560916900635\n",
      "Iteration 76\n",
      "loss = 123.20499420166016\n",
      "exp = 2.120271682739258\n",
      "Iteration 77\n",
      "loss = 116.49455261230469\n",
      "exp = 2.1144611835479736\n",
      "Iteration 78\n",
      "loss = 109.94884490966797\n",
      "exp = 2.108722686767578\n",
      "Iteration 79\n",
      "loss = 103.56170654296875\n",
      "exp = 2.1030542850494385\n",
      "Iteration 80\n",
      "loss = 97.32771301269531\n",
      "exp = 2.09745454788208\n",
      "Iteration 81\n",
      "loss = 91.24112701416016\n",
      "exp = 2.09192156791687\n",
      "Iteration 82\n",
      "loss = 85.29692077636719\n",
      "exp = 2.086453914642334\n",
      "Iteration 83\n",
      "loss = 79.48998260498047\n",
      "exp = 2.081049919128418\n",
      "Iteration 84\n",
      "loss = 73.81563568115234\n",
      "exp = 2.0757081508636475\n",
      "Iteration 85\n",
      "loss = 68.26936340332031\n",
      "exp = 2.070427179336548\n",
      "Iteration 86\n",
      "loss = 62.8468017578125\n",
      "exp = 2.0652055740356445\n",
      "Iteration 87\n",
      "loss = 57.54397964477539\n",
      "exp = 2.060042142868042\n",
      "Iteration 88\n",
      "loss = 52.35681915283203\n",
      "exp = 2.0549354553222656\n",
      "Iteration 89\n",
      "loss = 47.28160095214844\n",
      "exp = 2.04988431930542\n",
      "Iteration 90\n",
      "loss = 42.31474304199219\n",
      "exp = 2.0448875427246094\n",
      "Iteration 91\n",
      "loss = 37.452754974365234\n",
      "exp = 2.0399439334869385\n",
      "Iteration 92\n",
      "loss = 32.69252014160156\n",
      "exp = 2.035052537918091\n",
      "Iteration 93\n",
      "loss = 28.0305233001709\n",
      "exp = 2.030211925506592\n",
      "Iteration 94\n",
      "loss = 23.46407699584961\n",
      "exp = 2.025421380996704\n",
      "Iteration 95\n",
      "loss = 18.99009132385254\n",
      "exp = 2.0206797122955322\n",
      "Iteration 96\n",
      "loss = 14.605827331542969\n",
      "exp = 2.0159859657287598\n",
      "Iteration 97\n",
      "loss = 10.308375358581543\n",
      "exp = 2.011338949203491\n",
      "Iteration 98\n",
      "loss = 6.095371246337891\n",
      "exp = 2.0067379474639893\n",
      "Iteration 99\n",
      "loss = 1.9643033742904663\n",
      "exp = 2.0021820068359375\n",
      "Iteration 100\n",
      "loss = 2.0871756076812744\n",
      "exp = 1.997670292854309\n",
      "Iteration 101\n",
      "loss = 1.9251511096954346\n",
      "exp = 2.002138614654541\n",
      "Iteration 102\n",
      "loss = 2.125523090362549\n",
      "exp = 1.9976273775100708\n",
      "Iteration 103\n",
      "loss = 1.8860043287277222\n",
      "exp = 2.0020952224731445\n",
      "Iteration 104\n",
      "loss = 2.1638729572296143\n",
      "exp = 1.9975844621658325\n",
      "Iteration 105\n",
      "loss = 1.8470734357833862\n",
      "exp = 2.002052068710327\n",
      "Iteration 106\n",
      "loss = 2.2021071910858154\n",
      "exp = 1.9975416660308838\n",
      "Iteration 107\n",
      "loss = 1.8079333305358887\n",
      "exp = 2.0020086765289307\n",
      "Iteration 108\n",
      "loss = 2.2404448986053467\n",
      "exp = 1.9974987506866455\n",
      "Iteration 109\n",
      "loss = 1.7690099477767944\n",
      "exp = 2.0019655227661133\n",
      "Iteration 110\n",
      "loss = 2.2786779403686523\n",
      "exp = 1.9974559545516968\n",
      "Iteration 111\n",
      "loss = 1.7298728227615356\n",
      "exp = 2.001922130584717\n",
      "Iteration 112\n",
      "loss = 2.3170087337493896\n",
      "exp = 1.9974130392074585\n",
      "Iteration 113\n",
      "loss = 1.6909598112106323\n",
      "exp = 2.0018789768218994\n",
      "Iteration 114\n",
      "loss = 2.3552298545837402\n",
      "exp = 1.9973702430725098\n",
      "Iteration 115\n",
      "loss = 1.65183424949646\n",
      "exp = 2.001835584640503\n",
      "Iteration 116\n",
      "loss = 2.393557071685791\n",
      "exp = 1.9973273277282715\n",
      "Iteration 117\n",
      "loss = 1.612924575805664\n",
      "exp = 2.0017924308776855\n",
      "Iteration 118\n",
      "loss = 2.4317753314971924\n",
      "exp = 1.9972845315933228\n",
      "Iteration 119\n",
      "loss = 1.5740151405334473\n",
      "exp = 2.001749277114868\n",
      "Iteration 120\n",
      "loss = 2.4699859619140625\n",
      "exp = 1.997241735458374\n",
      "Iteration 121\n",
      "loss = 1.534903645515442\n",
      "exp = 2.0017058849334717\n",
      "Iteration 122\n",
      "loss = 2.5083014965057373\n",
      "exp = 1.9971988201141357\n",
      "Iteration 123\n",
      "loss = 1.4960078001022339\n",
      "exp = 2.0016627311706543\n",
      "Iteration 124\n",
      "loss = 2.546504020690918\n",
      "exp = 1.997156023979187\n",
      "Iteration 125\n",
      "loss = 1.4568971395492554\n",
      "exp = 2.001619338989258\n",
      "Iteration 126\n",
      "loss = 2.5848121643066406\n",
      "exp = 1.9971131086349487\n",
      "Iteration 127\n",
      "loss = 1.4180049896240234\n",
      "exp = 2.0015761852264404\n",
      "Iteration 128\n",
      "loss = 2.623011827468872\n",
      "exp = 1.9970703125\n",
      "Iteration 129\n",
      "loss = 1.3789045810699463\n",
      "exp = 2.001532793045044\n",
      "Iteration 130\n",
      "loss = 2.6613118648529053\n",
      "exp = 1.9970273971557617\n",
      "Iteration 131\n",
      "loss = 1.3400185108184814\n",
      "exp = 2.0014896392822266\n",
      "Iteration 132\n",
      "loss = 2.6995036602020264\n",
      "exp = 1.996984601020813\n",
      "Iteration 133\n",
      "loss = 1.301140546798706\n",
      "exp = 2.001446485519409\n",
      "Iteration 134\n",
      "loss = 2.7375869750976562\n",
      "exp = 1.9969419240951538\n",
      "Iteration 135\n",
      "loss = 1.2622634172439575\n",
      "exp = 2.001403331756592\n",
      "Iteration 136\n",
      "loss = 2.775768995285034\n",
      "exp = 1.996899127960205\n",
      "Iteration 137\n",
      "loss = 1.2233916521072388\n",
      "exp = 2.0013601779937744\n",
      "Iteration 138\n",
      "loss = 2.8138437271118164\n",
      "exp = 1.996856451034546\n",
      "Iteration 139\n",
      "loss = 1.1845217943191528\n",
      "exp = 2.001317024230957\n",
      "Iteration 140\n",
      "loss = 2.852022886276245\n",
      "exp = 1.9968136548995972\n",
      "Iteration 141\n",
      "loss = 1.1456584930419922\n",
      "exp = 2.0012738704681396\n",
      "Iteration 142\n",
      "loss = 2.8900880813598633\n",
      "exp = 1.996770977973938\n",
      "Iteration 143\n",
      "loss = 1.1067979335784912\n",
      "exp = 2.0012307167053223\n",
      "Iteration 144\n",
      "loss = 2.928256034851074\n",
      "exp = 1.9967281818389893\n",
      "Iteration 145\n",
      "loss = 1.0679404735565186\n",
      "exp = 2.001187562942505\n",
      "Iteration 146\n",
      "loss = 2.9663162231445312\n",
      "exp = 1.99668550491333\n",
      "Iteration 147\n",
      "loss = 1.0290862321853638\n",
      "exp = 2.0011444091796875\n",
      "Iteration 148\n",
      "loss = 3.0044796466827393\n",
      "exp = 1.9966427087783813\n",
      "Iteration 149\n",
      "loss = 0.9902322292327881\n",
      "exp = 2.00110125541687\n",
      "Iteration 150\n",
      "loss = 3.042534112930298\n",
      "exp = 1.9966000318527222\n",
      "Iteration 151\n",
      "loss = 0.951382040977478\n",
      "exp = 2.0010581016540527\n",
      "Iteration 152\n",
      "loss = 3.0806922912597656\n",
      "exp = 1.9965572357177734\n",
      "Iteration 153\n",
      "loss = 0.9125458002090454\n",
      "exp = 2.0010149478912354\n",
      "Iteration 154\n",
      "loss = 3.1187374591827393\n",
      "exp = 1.9965145587921143\n",
      "Iteration 155\n",
      "loss = 0.8737033605575562\n",
      "exp = 2.000971794128418\n",
      "Iteration 156\n",
      "loss = 3.1568875312805176\n",
      "exp = 1.9964717626571655\n",
      "Iteration 157\n",
      "loss = 0.8348639607429504\n",
      "exp = 2.0009286403656006\n",
      "Iteration 158\n",
      "loss = 3.194924831390381\n",
      "exp = 1.9964290857315063\n",
      "Iteration 159\n",
      "loss = 0.7960334420204163\n",
      "exp = 2.000885486602783\n",
      "Iteration 160\n",
      "loss = 3.2330691814422607\n",
      "exp = 1.9963862895965576\n",
      "Iteration 161\n",
      "loss = 0.7572057247161865\n",
      "exp = 2.000842332839966\n",
      "Iteration 162\n",
      "loss = 3.2711000442504883\n",
      "exp = 1.9963436126708984\n",
      "Iteration 163\n",
      "loss = 0.7183753848075867\n",
      "exp = 2.0007991790771484\n",
      "Iteration 164\n",
      "loss = 3.309231996536255\n",
      "exp = 1.9963008165359497\n",
      "Iteration 165\n",
      "loss = 0.6795542240142822\n",
      "exp = 2.000756025314331\n",
      "Iteration 166\n",
      "loss = 3.3472626209259033\n",
      "exp = 1.9962581396102905\n",
      "Iteration 167\n",
      "loss = 0.6407375931739807\n",
      "exp = 2.0007128715515137\n",
      "Iteration 168\n",
      "loss = 3.385390520095825\n",
      "exp = 1.9962153434753418\n",
      "Iteration 169\n",
      "loss = 0.6019205451011658\n",
      "exp = 2.0006697177886963\n",
      "Iteration 170\n",
      "loss = 3.4234063625335693\n",
      "exp = 1.9961726665496826\n",
      "Iteration 171\n",
      "loss = 0.563326895236969\n",
      "exp = 2.000626802444458\n",
      "Iteration 172\n",
      "loss = 3.461317300796509\n",
      "exp = 1.996130108833313\n",
      "Iteration 173\n",
      "loss = 0.5245198607444763\n",
      "exp = 2.0005836486816406\n",
      "Iteration 174\n",
      "loss = 3.4993295669555664\n",
      "exp = 1.9960874319076538\n",
      "Iteration 175\n",
      "loss = 0.48592501878738403\n",
      "exp = 2.0005407333374023\n",
      "Iteration 176\n",
      "loss = 3.537228584289551\n",
      "exp = 1.9960448741912842\n",
      "Iteration 177\n",
      "loss = 0.44712793827056885\n",
      "exp = 2.000497579574585\n",
      "Iteration 178\n",
      "loss = 3.575230836868286\n",
      "exp = 1.996002197265625\n",
      "Iteration 179\n",
      "loss = 0.4085436463356018\n",
      "exp = 2.0004546642303467\n",
      "Iteration 180\n",
      "loss = 3.6131303310394287\n",
      "exp = 1.9959596395492554\n",
      "Iteration 181\n",
      "loss = 0.3699645400047302\n",
      "exp = 2.0004117488861084\n",
      "Iteration 182\n",
      "loss = 3.6509177684783936\n",
      "exp = 1.9959172010421753\n",
      "Iteration 183\n",
      "loss = 0.3313896059989929\n",
      "exp = 2.00036883354187\n",
      "Iteration 184\n",
      "loss = 3.6888065338134766\n",
      "exp = 1.9958746433258057\n",
      "Iteration 185\n",
      "loss = 0.2928203046321869\n",
      "exp = 2.000325918197632\n",
      "Iteration 186\n",
      "loss = 3.726686954498291\n",
      "exp = 1.995832085609436\n",
      "Iteration 187\n",
      "loss = 0.25403380393981934\n",
      "exp = 2.0002827644348145\n",
      "Iteration 188\n",
      "loss = 3.7646708488464355\n",
      "exp = 1.9957894086837769\n",
      "Iteration 189\n",
      "loss = 0.21546697616577148\n",
      "exp = 2.000239849090576\n",
      "Iteration 190\n",
      "loss = 3.802554130554199\n",
      "exp = 1.9957468509674072\n",
      "Iteration 191\n",
      "loss = 0.1769118458032608\n",
      "exp = 2.000196933746338\n",
      "Iteration 192\n",
      "loss = 3.8403210639953613\n",
      "exp = 1.9957044124603271\n",
      "Iteration 193\n",
      "loss = 0.13834820687770844\n",
      "exp = 2.0001540184020996\n",
      "Iteration 194\n",
      "loss = 3.8781914710998535\n",
      "exp = 1.9956618547439575\n",
      "Iteration 195\n",
      "loss = 0.09979522228240967\n",
      "exp = 2.0001111030578613\n",
      "Iteration 196\n",
      "loss = 3.91595721244812\n",
      "exp = 1.9956194162368774\n",
      "Iteration 197\n",
      "loss = 0.06124330684542656\n",
      "exp = 2.000068187713623\n",
      "Iteration 198\n",
      "loss = 3.953817129135132\n",
      "exp = 1.9955768585205078\n",
      "Iteration 199\n",
      "loss = 0.022694068029522896\n",
      "exp = 2.0000252723693848\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.sqrt(torch.mean((y - y_hat).pow(2).sum()))\n",
    "\n",
    "def forward(x, e):\n",
    "    \"\"\"Forward pass for our function\"\"\"\n",
    "    return x.pow(e.repeat(x.size(0)))\n",
    "\n",
    "# Let's define some settings\n",
    "n = 100 # number of examples\n",
    "learning_rate = 5e-6\n",
    "\n",
    "# Model definition\n",
    "x = Variable(torch.rand(n) * 10, requires_grad=False)\n",
    "\n",
    "# Model parameter and it's true value\n",
    "exp = Variable(torch.FloatTensor([2.0]), requires_grad=False)\n",
    "exp_hat = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "y = forward(x, exp)\n",
    "\n",
    "loss_history = []\n",
    "exp_history = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(0, 200):\n",
    "    print(\"Iteration %d\" % i)\n",
    "    \n",
    "    # Compute current estimate\n",
    "    y_hat = forward(x, exp_hat)\n",
    "    \n",
    "    # Calculate loss function\n",
    "    loss = rmse(y, y_hat)\n",
    "    \n",
    "    # Do some recordings for plots\n",
    "    loss_history.append(loss.data[0])\n",
    "    exp_history.append(y_hat.data[0])\n",
    "    \n",
    "    # Compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    print(\"loss = %s\" % loss.data[0])\n",
    "    print(\"exp = %s\" % exp_hat.data[0])\n",
    "    \n",
    "    # Update model parameters\n",
    "    exp_hat.data -= learning_rate * exp_hat.grad.data\n",
    "    exp_hat.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmYAAAGrCAYAAABqslt9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwbkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11398bc18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def plot_results(loss_history, exp_history):\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    fig.add_axes()\n",
    "\n",
    "    ax1 = fig.add_subplot(121)\n",
    "    ax2 = fig.add_subplot(122)\n",
    "\n",
    "    for ax in [ax1, ax2]:\n",
    "        ax.spines[\"top\"].set_visible(False)\n",
    "        ax.spines[\"right\"].set_visible(False)\n",
    "        ax.spines[\"left\"].set_visible(False)\n",
    "        ax.spines[\"bottom\"].set_visible(False)\n",
    "        ax.grid(color='b', linestyle='--', linewidth=0.5, alpha=0.3)\n",
    "        ax.tick_params(direction='out', color='b', width='2')\n",
    "        \n",
    "    ax1.set_title('RMSE Loss')\n",
    "    ax2.set_title('exp')\n",
    "    ax1.plot(np.arange(len(loss_history)), loss_history)\n",
    "    ax2.plot(np.arange(len(exp_history)), exp_history)\n",
    "    \n",
    "plot_results(loss_history, exp_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch with optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return torch.sqrt(torch.mean((y - y_hat).pow(2)))\n",
    "\n",
    "def forward(x, e):\n",
    "    \"\"\"Forward pass for our function\"\"\"\n",
    "    return x.pow(e.repeat(x.size(0)))\n",
    "\n",
    "# Let's define some settings\n",
    "n = 1000 # number of examples\n",
    "learning_rate = 5e-10\n",
    "\n",
    "# Model definition\n",
    "x = Variable(torch.rand(n) * 10, requires_grad=False)\n",
    "y = forward(x, exp)\n",
    "\n",
    "# Model parameters\n",
    "exp = Variable(torch.FloatTensor([2.0]), requires_grad=False)\n",
    "exp_hat = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "\n",
    "# Optimizer (NEW)\n",
    "opt = torch.optim.SGD([exp_hat], lr=learning_rate, momentum=0.9)\n",
    "\n",
    "loss_history = []\n",
    "exp_history = []\n",
    "\n",
    "# Training loop\n",
    "for i in range(0, 10000):\n",
    "    opt.zero_grad()\n",
    "    print(\"Iteration %d\" % i)\n",
    "    \n",
    "    # Compute current estimate\n",
    "    y_hat = forward(x, exp_hat)\n",
    "    \n",
    "    # Calculate loss function\n",
    "    loss = rmse(y, y_hat)\n",
    "    \n",
    "    # Do some recordings for plots\n",
    "    loss_history.append(loss.data[0])\n",
    "    exp_history.append(y_hat.data[0])\n",
    "    \n",
    "    # Update model parameters\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    print(\"loss = %s\" % loss.data[0])\n",
    "    print(\"exp = %s\" % exp_hat.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(loss_history, exp_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square((y - y_hat))))\n",
    "\n",
    "def forward(x, e):\n",
    "    \"\"\"Forward pass for our function\"\"\"\n",
    "    # tensorflow has automatic broadcasting \n",
    "    # so we do not need to reshape e manually\n",
    "    return tf.pow(x, e) \n",
    "\n",
    "n = 100 # number of examples\n",
    "learning_rate = 5e-6\n",
    "\n",
    "# Placeholders for data\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model parameters\n",
    "exp = tf.constant(2.0)\n",
    "exp_hat = tf.Variable(4.0, name='exp_hat')\n",
    "\n",
    "# Model definition\n",
    "y_hat = forward(x, exp_hat)\n",
    "\n",
    "# Optimizer\n",
    "loss = rmse(y, y_hat)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# We will run this operation to perform a single training step,\n",
    "# e.g. opt.step() in Pytorch.\n",
    "# Execution of this operation will also update model parameters\n",
    "train_op = opt.minimize(loss) \n",
    "\n",
    "# Let's generate some training data\n",
    "x_train = np.random.rand(n) + 10\n",
    "y_train = x_train ** 2\n",
    "\n",
    "loss_history = []\n",
    "exp_history = []\n",
    "\n",
    "# First, we need to create a Tensorflow session object\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all defined variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(0, 500):\n",
    "        print(\"Iteration %d\" % i)\n",
    "        # Run a single trainig step\n",
    "        curr_loss, curr_exp, _ = sess.run([loss, exp_hat, train_op], feed_dict={x: x_train, y: y_train})\n",
    "        \n",
    "        print(\"loss = %s\" % curr_loss)\n",
    "        print(\"exp = %s\" % curr_exp)\n",
    "        \n",
    "        # Do some recordings for plots\n",
    "        loss_history.append(curr_loss)\n",
    "        exp_history.append(curr_exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_results(loss_history, exp_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's introduce the Tensorboard. This tool is very useful for debugging and comparison of different training runs. For example, you can train a model then tune some hyperparameters and train it again. Both runs can be displayed at Tensorboard simultaneously to indicate possible differences. Tensorboard can:\n",
    "- Display model graph\n",
    "- Plot sclarar variables\n",
    "- Visualize disbtibutions and histograms\n",
    "- Visualize images\n",
    "- Visualize embeddings\n",
    "- Play audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "loss = 12361.6\n",
      "exp = 3.85277\n",
      "Iteration 1\n",
      "loss = 8699.6\n",
      "exp = 3.74877\n",
      "Iteration 2\n",
      "loss = 6781.7\n",
      "exp = 3.66742\n",
      "Iteration 3\n",
      "loss = 5577.4\n",
      "exp = 3.60028\n",
      "Iteration 4\n",
      "loss = 4743.76\n",
      "exp = 3.54299\n",
      "Iteration 5\n",
      "loss = 4129.67\n",
      "exp = 3.49294\n",
      "Iteration 6\n",
      "loss = 3657.2\n",
      "exp = 3.44847\n",
      "Iteration 7\n",
      "loss = 3281.74\n",
      "exp = 3.40843\n",
      "Iteration 8\n",
      "loss = 2975.81\n",
      "exp = 3.37201\n",
      "Iteration 9\n",
      "loss = 2721.51\n",
      "exp = 3.33858\n",
      "Iteration 10\n",
      "loss = 2506.64\n",
      "exp = 3.30769\n",
      "Iteration 11\n",
      "loss = 2322.6\n",
      "exp = 3.27897\n",
      "Iteration 12\n",
      "loss = 2163.12\n",
      "exp = 3.25214\n",
      "Iteration 13\n",
      "loss = 2023.56\n",
      "exp = 3.22695\n",
      "Iteration 14\n",
      "loss = 1900.36\n",
      "exp = 3.20321\n",
      "Iteration 15\n",
      "loss = 1790.79\n",
      "exp = 3.18077\n",
      "Iteration 16\n",
      "loss = 1692.69\n",
      "exp = 3.15949\n",
      "Iteration 17\n",
      "loss = 1604.33\n",
      "exp = 3.13925\n",
      "Iteration 18\n",
      "loss = 1524.31\n",
      "exp = 3.11995\n",
      "Iteration 19\n",
      "loss = 1451.51\n",
      "exp = 3.10151\n",
      "Iteration 20\n",
      "loss = 1384.98\n",
      "exp = 3.08386\n",
      "Iteration 21\n",
      "loss = 1323.93\n",
      "exp = 3.06693\n",
      "Iteration 22\n",
      "loss = 1267.72\n",
      "exp = 3.05066\n",
      "Iteration 23\n",
      "loss = 1215.79\n",
      "exp = 3.035\n",
      "Iteration 24\n",
      "loss = 1167.66\n",
      "exp = 3.01991\n",
      "Iteration 25\n",
      "loss = 1122.93\n",
      "exp = 3.00535\n",
      "Iteration 26\n",
      "loss = 1081.25\n",
      "exp = 2.99129\n",
      "Iteration 27\n",
      "loss = 1042.32\n",
      "exp = 2.97768\n",
      "Iteration 28\n",
      "loss = 1005.86\n",
      "exp = 2.9645\n",
      "Iteration 29\n",
      "loss = 971.66\n",
      "exp = 2.95172\n",
      "Iteration 30\n",
      "loss = 939.505\n",
      "exp = 2.93933\n",
      "Iteration 31\n",
      "loss = 909.217\n",
      "exp = 2.92729\n",
      "Iteration 32\n",
      "loss = 880.639\n",
      "exp = 2.91559\n",
      "Iteration 33\n",
      "loss = 853.628\n",
      "exp = 2.9042\n",
      "Iteration 34\n",
      "loss = 828.06\n",
      "exp = 2.89312\n",
      "Iteration 35\n",
      "loss = 803.819\n",
      "exp = 2.88233\n",
      "Iteration 36\n",
      "loss = 780.807\n",
      "exp = 2.8718\n",
      "Iteration 37\n",
      "loss = 758.93\n",
      "exp = 2.86154\n",
      "Iteration 38\n",
      "loss = 738.107\n",
      "exp = 2.85152\n",
      "Iteration 39\n",
      "loss = 718.264\n",
      "exp = 2.84173\n",
      "Iteration 40\n",
      "loss = 699.331\n",
      "exp = 2.83217\n",
      "Iteration 41\n",
      "loss = 681.249\n",
      "exp = 2.82282\n",
      "Iteration 42\n",
      "loss = 663.96\n",
      "exp = 2.81368\n",
      "Iteration 43\n",
      "loss = 647.413\n",
      "exp = 2.80473\n",
      "Iteration 44\n",
      "loss = 631.561\n",
      "exp = 2.79596\n",
      "Iteration 45\n",
      "loss = 616.362\n",
      "exp = 2.78738\n",
      "Iteration 46\n",
      "loss = 601.775\n",
      "exp = 2.77897\n",
      "Iteration 47\n",
      "loss = 587.765\n",
      "exp = 2.77072\n",
      "Iteration 48\n",
      "loss = 574.297\n",
      "exp = 2.76264\n",
      "Iteration 49\n",
      "loss = 561.341\n",
      "exp = 2.7547\n",
      "Iteration 50\n",
      "loss = 548.867\n",
      "exp = 2.74691\n",
      "Iteration 51\n",
      "loss = 536.85\n",
      "exp = 2.73927\n",
      "Iteration 52\n",
      "loss = 525.265\n",
      "exp = 2.73176\n",
      "Iteration 53\n",
      "loss = 514.088\n",
      "exp = 2.72438\n",
      "Iteration 54\n",
      "loss = 503.299\n",
      "exp = 2.71713\n",
      "Iteration 55\n",
      "loss = 492.877\n",
      "exp = 2.71001\n",
      "Iteration 56\n",
      "loss = 482.805\n",
      "exp = 2.703\n",
      "Iteration 57\n",
      "loss = 473.064\n",
      "exp = 2.69611\n",
      "Iteration 58\n",
      "loss = 463.639\n",
      "exp = 2.68933\n",
      "Iteration 59\n",
      "loss = 454.514\n",
      "exp = 2.68265\n",
      "Iteration 60\n",
      "loss = 445.676\n",
      "exp = 2.67608\n",
      "Iteration 61\n",
      "loss = 437.11\n",
      "exp = 2.66961\n",
      "Iteration 62\n",
      "loss = 428.805\n",
      "exp = 2.66324\n",
      "Iteration 63\n",
      "loss = 420.749\n",
      "exp = 2.65697\n",
      "Iteration 64\n",
      "loss = 412.93\n",
      "exp = 2.65078\n",
      "Iteration 65\n",
      "loss = 405.339\n",
      "exp = 2.64469\n",
      "Iteration 66\n",
      "loss = 397.965\n",
      "exp = 2.63868\n",
      "Iteration 67\n",
      "loss = 390.8\n",
      "exp = 2.63276\n",
      "Iteration 68\n",
      "loss = 383.834\n",
      "exp = 2.62692\n",
      "Iteration 69\n",
      "loss = 377.059\n",
      "exp = 2.62116\n",
      "Iteration 70\n",
      "loss = 370.468\n",
      "exp = 2.61548\n",
      "Iteration 71\n",
      "loss = 364.053\n",
      "exp = 2.60987\n",
      "Iteration 72\n",
      "loss = 357.807\n",
      "exp = 2.60434\n",
      "Iteration 73\n",
      "loss = 351.724\n",
      "exp = 2.59888\n",
      "Iteration 74\n",
      "loss = 345.797\n",
      "exp = 2.59349\n",
      "Iteration 75\n",
      "loss = 340.021\n",
      "exp = 2.58816\n",
      "Iteration 76\n",
      "loss = 334.389\n",
      "exp = 2.58291\n",
      "Iteration 77\n",
      "loss = 328.896\n",
      "exp = 2.57771\n",
      "Iteration 78\n",
      "loss = 323.538\n",
      "exp = 2.57258\n",
      "Iteration 79\n",
      "loss = 318.308\n",
      "exp = 2.56752\n",
      "Iteration 80\n",
      "loss = 313.204\n",
      "exp = 2.56251\n",
      "Iteration 81\n",
      "loss = 308.219\n",
      "exp = 2.55756\n",
      "Iteration 82\n",
      "loss = 303.351\n",
      "exp = 2.55267\n",
      "Iteration 83\n",
      "loss = 298.595\n",
      "exp = 2.54784\n",
      "Iteration 84\n",
      "loss = 293.946\n",
      "exp = 2.54306\n",
      "Iteration 85\n",
      "loss = 289.403\n",
      "exp = 2.53833\n",
      "Iteration 86\n",
      "loss = 284.96\n",
      "exp = 2.53366\n",
      "Iteration 87\n",
      "loss = 280.616\n",
      "exp = 2.52903\n",
      "Iteration 88\n",
      "loss = 276.365\n",
      "exp = 2.52446\n",
      "Iteration 89\n",
      "loss = 272.206\n",
      "exp = 2.51994\n",
      "Iteration 90\n",
      "loss = 268.136\n",
      "exp = 2.51546\n",
      "Iteration 91\n",
      "loss = 264.151\n",
      "exp = 2.51104\n",
      "Iteration 92\n",
      "loss = 260.25\n",
      "exp = 2.50665\n",
      "Iteration 93\n",
      "loss = 256.429\n",
      "exp = 2.50232\n",
      "Iteration 94\n",
      "loss = 252.686\n",
      "exp = 2.49802\n",
      "Iteration 95\n",
      "loss = 249.018\n",
      "exp = 2.49377\n",
      "Iteration 96\n",
      "loss = 245.424\n",
      "exp = 2.48957\n",
      "Iteration 97\n",
      "loss = 241.901\n",
      "exp = 2.4854\n",
      "Iteration 98\n",
      "loss = 238.447\n",
      "exp = 2.48127\n",
      "Iteration 99\n",
      "loss = 235.061\n",
      "exp = 2.47719\n",
      "Iteration 100\n",
      "loss = 231.739\n",
      "exp = 2.47314\n",
      "Iteration 101\n",
      "loss = 228.481\n",
      "exp = 2.46914\n",
      "Iteration 102\n",
      "loss = 225.284\n",
      "exp = 2.46517\n",
      "Iteration 103\n",
      "loss = 222.146\n",
      "exp = 2.46123\n",
      "Iteration 104\n",
      "loss = 219.067\n",
      "exp = 2.45734\n",
      "Iteration 105\n",
      "loss = 216.045\n",
      "exp = 2.45348\n",
      "Iteration 106\n",
      "loss = 213.077\n",
      "exp = 2.44965\n",
      "Iteration 107\n",
      "loss = 210.163\n",
      "exp = 2.44586\n",
      "Iteration 108\n",
      "loss = 207.301\n",
      "exp = 2.4421\n",
      "Iteration 109\n",
      "loss = 204.489\n",
      "exp = 2.43838\n",
      "Iteration 110\n",
      "loss = 201.727\n",
      "exp = 2.43468\n",
      "Iteration 111\n",
      "loss = 199.012\n",
      "exp = 2.43102\n",
      "Iteration 112\n",
      "loss = 196.345\n",
      "exp = 2.4274\n",
      "Iteration 113\n",
      "loss = 193.723\n",
      "exp = 2.4238\n",
      "Iteration 114\n",
      "loss = 191.146\n",
      "exp = 2.42023\n",
      "Iteration 115\n",
      "loss = 188.611\n",
      "exp = 2.41669\n",
      "Iteration 116\n",
      "loss = 186.119\n",
      "exp = 2.41319\n",
      "Iteration 117\n",
      "loss = 183.669\n",
      "exp = 2.40971\n",
      "Iteration 118\n",
      "loss = 181.258\n",
      "exp = 2.40626\n",
      "Iteration 119\n",
      "loss = 178.886\n",
      "exp = 2.40283\n",
      "Iteration 120\n",
      "loss = 176.553\n",
      "exp = 2.39944\n",
      "Iteration 121\n",
      "loss = 174.257\n",
      "exp = 2.39607\n",
      "Iteration 122\n",
      "loss = 171.997\n",
      "exp = 2.39273\n",
      "Iteration 123\n",
      "loss = 169.773\n",
      "exp = 2.38942\n",
      "Iteration 124\n",
      "loss = 167.584\n",
      "exp = 2.38613\n",
      "Iteration 125\n",
      "loss = 165.429\n",
      "exp = 2.38286\n",
      "Iteration 126\n",
      "loss = 163.306\n",
      "exp = 2.37962\n",
      "Iteration 127\n",
      "loss = 161.217\n",
      "exp = 2.37641\n",
      "Iteration 128\n",
      "loss = 159.158\n",
      "exp = 2.37322\n",
      "Iteration 129\n",
      "loss = 157.131\n",
      "exp = 2.37005\n",
      "Iteration 130\n",
      "loss = 155.134\n",
      "exp = 2.36691\n",
      "Iteration 131\n",
      "loss = 153.166\n",
      "exp = 2.36379\n",
      "Iteration 132\n",
      "loss = 151.227\n",
      "exp = 2.3607\n",
      "Iteration 133\n",
      "loss = 149.317\n",
      "exp = 2.35762\n",
      "Iteration 134\n",
      "loss = 147.434\n",
      "exp = 2.35457\n",
      "Iteration 135\n",
      "loss = 145.579\n",
      "exp = 2.35154\n",
      "Iteration 136\n",
      "loss = 143.749\n",
      "exp = 2.34853\n",
      "Iteration 137\n",
      "loss = 141.946\n",
      "exp = 2.34555\n",
      "Iteration 138\n",
      "loss = 140.168\n",
      "exp = 2.34258\n",
      "Iteration 139\n",
      "loss = 138.415\n",
      "exp = 2.33963\n",
      "Iteration 140\n",
      "loss = 136.686\n",
      "exp = 2.33671\n",
      "Iteration 141\n",
      "loss = 134.981\n",
      "exp = 2.33381\n",
      "Iteration 142\n",
      "loss = 133.3\n",
      "exp = 2.33092\n",
      "Iteration 143\n",
      "loss = 131.641\n",
      "exp = 2.32806\n",
      "Iteration 144\n",
      "loss = 130.004\n",
      "exp = 2.32521\n",
      "Iteration 145\n",
      "loss = 128.39\n",
      "exp = 2.32238\n",
      "Iteration 146\n",
      "loss = 126.797\n",
      "exp = 2.31957\n",
      "Iteration 147\n",
      "loss = 125.225\n",
      "exp = 2.31678\n",
      "Iteration 148\n",
      "loss = 123.674\n",
      "exp = 2.31401\n",
      "Iteration 149\n",
      "loss = 122.143\n",
      "exp = 2.31126\n",
      "Iteration 150\n",
      "loss = 120.631\n",
      "exp = 2.30852\n",
      "Iteration 151\n",
      "loss = 119.14\n",
      "exp = 2.30581\n",
      "Iteration 152\n",
      "loss = 117.667\n",
      "exp = 2.30311\n",
      "Iteration 153\n",
      "loss = 116.213\n",
      "exp = 2.30042\n",
      "Iteration 154\n",
      "loss = 114.778\n",
      "exp = 2.29776\n",
      "Iteration 155\n",
      "loss = 113.36\n",
      "exp = 2.29511\n",
      "Iteration 156\n",
      "loss = 111.961\n",
      "exp = 2.29247\n",
      "Iteration 157\n",
      "loss = 110.578\n",
      "exp = 2.28986\n",
      "Iteration 158\n",
      "loss = 109.213\n",
      "exp = 2.28726\n",
      "Iteration 159\n",
      "loss = 107.864\n",
      "exp = 2.28467\n",
      "Iteration 160\n",
      "loss = 106.532\n",
      "exp = 2.2821\n",
      "Iteration 161\n",
      "loss = 105.216\n",
      "exp = 2.27955\n",
      "Iteration 162\n",
      "loss = 103.916\n",
      "exp = 2.27701\n",
      "Iteration 163\n",
      "loss = 102.631\n",
      "exp = 2.27449\n",
      "Iteration 164\n",
      "loss = 101.361\n",
      "exp = 2.27198\n",
      "Iteration 165\n",
      "loss = 100.107\n",
      "exp = 2.26949\n",
      "Iteration 166\n",
      "loss = 98.8672\n",
      "exp = 2.26701\n",
      "Iteration 167\n",
      "loss = 97.6419\n",
      "exp = 2.26454\n",
      "Iteration 168\n",
      "loss = 96.4309\n",
      "exp = 2.26209\n",
      "Iteration 169\n",
      "loss = 95.2339\n",
      "exp = 2.25966\n",
      "Iteration 170\n",
      "loss = 94.0506\n",
      "exp = 2.25723\n",
      "Iteration 171\n",
      "loss = 92.8808\n",
      "exp = 2.25483\n",
      "Iteration 172\n",
      "loss = 91.7243\n",
      "exp = 2.25243\n",
      "Iteration 173\n",
      "loss = 90.5809\n",
      "exp = 2.25005\n",
      "Iteration 174\n",
      "loss = 89.4502\n",
      "exp = 2.24768\n",
      "Iteration 175\n",
      "loss = 88.3321\n",
      "exp = 2.24533\n",
      "Iteration 176\n",
      "loss = 87.2265\n",
      "exp = 2.24299\n",
      "Iteration 177\n",
      "loss = 86.1331\n",
      "exp = 2.24066\n",
      "Iteration 178\n",
      "loss = 85.0516\n",
      "exp = 2.23834\n",
      "Iteration 179\n",
      "loss = 83.9819\n",
      "exp = 2.23604\n",
      "Iteration 180\n",
      "loss = 82.9239\n",
      "exp = 2.23375\n",
      "Iteration 181\n",
      "loss = 81.8773\n",
      "exp = 2.23147\n",
      "Iteration 182\n",
      "loss = 80.8419\n",
      "exp = 2.2292\n",
      "Iteration 183\n",
      "loss = 79.8176\n",
      "exp = 2.22695\n",
      "Iteration 184\n",
      "loss = 78.8041\n",
      "exp = 2.22471\n",
      "Iteration 185\n",
      "loss = 77.8014\n",
      "exp = 2.22248\n",
      "Iteration 186\n",
      "loss = 76.8092\n",
      "exp = 2.22026\n",
      "Iteration 187\n",
      "loss = 75.8274\n",
      "exp = 2.21805\n",
      "Iteration 188\n",
      "loss = 74.8558\n",
      "exp = 2.21586\n",
      "Iteration 189\n",
      "loss = 73.8942\n",
      "exp = 2.21367\n",
      "Iteration 190\n",
      "loss = 72.9425\n",
      "exp = 2.2115\n",
      "Iteration 191\n",
      "loss = 72.0006\n",
      "exp = 2.20934\n",
      "Iteration 192\n",
      "loss = 71.0683\n",
      "exp = 2.20719\n",
      "Iteration 193\n",
      "loss = 70.1454\n",
      "exp = 2.20505\n",
      "Iteration 194\n",
      "loss = 69.2318\n",
      "exp = 2.20292\n",
      "Iteration 195\n",
      "loss = 68.3273\n",
      "exp = 2.2008\n",
      "Iteration 196\n",
      "loss = 67.4319\n",
      "exp = 2.19869\n",
      "Iteration 197\n",
      "loss = 66.5454\n",
      "exp = 2.19659\n",
      "Iteration 198\n",
      "loss = 65.6676\n",
      "exp = 2.19451\n",
      "Iteration 199\n",
      "loss = 64.7986\n",
      "exp = 2.19243\n",
      "Iteration 200\n",
      "loss = 63.938\n",
      "exp = 2.19036\n",
      "Iteration 201\n",
      "loss = 63.0857\n",
      "exp = 2.18831\n",
      "Iteration 202\n",
      "loss = 62.2418\n",
      "exp = 2.18626\n",
      "Iteration 203\n",
      "loss = 61.4059\n",
      "exp = 2.18422\n",
      "Iteration 204\n",
      "loss = 60.5782\n",
      "exp = 2.1822\n",
      "Iteration 205\n",
      "loss = 59.7583\n",
      "exp = 2.18018\n",
      "Iteration 206\n",
      "loss = 58.9462\n",
      "exp = 2.17817\n",
      "Iteration 207\n",
      "loss = 58.1417\n",
      "exp = 2.17617\n",
      "Iteration 208\n",
      "loss = 57.3449\n",
      "exp = 2.17418\n",
      "Iteration 209\n",
      "loss = 56.5555\n",
      "exp = 2.1722\n",
      "Iteration 210\n",
      "loss = 55.7735\n",
      "exp = 2.17023\n",
      "Iteration 211\n",
      "loss = 54.9987\n",
      "exp = 2.16827\n",
      "Iteration 212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss = 54.2312\n",
      "exp = 2.16632\n",
      "Iteration 213\n",
      "loss = 53.4707\n",
      "exp = 2.16438\n",
      "Iteration 214\n",
      "loss = 52.7171\n",
      "exp = 2.16244\n",
      "Iteration 215\n",
      "loss = 51.9703\n",
      "exp = 2.16052\n",
      "Iteration 216\n",
      "loss = 51.2304\n",
      "exp = 2.1586\n",
      "Iteration 217\n",
      "loss = 50.4972\n",
      "exp = 2.15669\n",
      "Iteration 218\n",
      "loss = 49.7705\n",
      "exp = 2.15479\n",
      "Iteration 219\n",
      "loss = 49.0503\n",
      "exp = 2.1529\n",
      "Iteration 220\n",
      "loss = 48.3366\n",
      "exp = 2.15102\n",
      "Iteration 221\n",
      "loss = 47.6292\n",
      "exp = 2.14914\n",
      "Iteration 222\n",
      "loss = 46.9281\n",
      "exp = 2.14728\n",
      "Iteration 223\n",
      "loss = 46.2331\n",
      "exp = 2.14542\n",
      "Iteration 224\n",
      "loss = 45.5442\n",
      "exp = 2.14357\n",
      "Iteration 225\n",
      "loss = 44.8613\n",
      "exp = 2.14173\n",
      "Iteration 226\n",
      "loss = 44.1843\n",
      "exp = 2.13989\n",
      "Iteration 227\n",
      "loss = 43.5132\n",
      "exp = 2.13807\n",
      "Iteration 228\n",
      "loss = 42.8479\n",
      "exp = 2.13625\n",
      "Iteration 229\n",
      "loss = 42.1883\n",
      "exp = 2.13444\n",
      "Iteration 230\n",
      "loss = 41.5342\n",
      "exp = 2.13264\n",
      "Iteration 231\n",
      "loss = 40.8858\n",
      "exp = 2.13084\n",
      "Iteration 232\n",
      "loss = 40.2429\n",
      "exp = 2.12905\n",
      "Iteration 233\n",
      "loss = 39.6053\n",
      "exp = 2.12727\n",
      "Iteration 234\n",
      "loss = 38.9731\n",
      "exp = 2.1255\n",
      "Iteration 235\n",
      "loss = 38.3461\n",
      "exp = 2.12374\n",
      "Iteration 236\n",
      "loss = 37.7244\n",
      "exp = 2.12198\n",
      "Iteration 237\n",
      "loss = 37.1078\n",
      "exp = 2.12023\n",
      "Iteration 238\n",
      "loss = 36.4964\n",
      "exp = 2.11849\n",
      "Iteration 239\n",
      "loss = 35.8899\n",
      "exp = 2.11675\n",
      "Iteration 240\n",
      "loss = 35.2885\n",
      "exp = 2.11502\n",
      "Iteration 241\n",
      "loss = 34.6919\n",
      "exp = 2.1133\n",
      "Iteration 242\n",
      "loss = 34.1001\n",
      "exp = 2.11158\n",
      "Iteration 243\n",
      "loss = 33.5131\n",
      "exp = 2.10988\n",
      "Iteration 244\n",
      "loss = 32.9309\n",
      "exp = 2.10818\n",
      "Iteration 245\n",
      "loss = 32.3533\n",
      "exp = 2.10648\n",
      "Iteration 246\n",
      "loss = 31.7803\n",
      "exp = 2.10479\n",
      "Iteration 247\n",
      "loss = 31.2119\n",
      "exp = 2.10311\n",
      "Iteration 248\n",
      "loss = 30.648\n",
      "exp = 2.10144\n",
      "Iteration 249\n",
      "loss = 30.0885\n",
      "exp = 2.09977\n",
      "Iteration 250\n",
      "loss = 29.5335\n",
      "exp = 2.09811\n",
      "Iteration 251\n",
      "loss = 28.9828\n",
      "exp = 2.09646\n",
      "Iteration 252\n",
      "loss = 28.4363\n",
      "exp = 2.09481\n",
      "Iteration 253\n",
      "loss = 27.8941\n",
      "exp = 2.09317\n",
      "Iteration 254\n",
      "loss = 27.3561\n",
      "exp = 2.09153\n",
      "Iteration 255\n",
      "loss = 26.8223\n",
      "exp = 2.0899\n",
      "Iteration 256\n",
      "loss = 26.2925\n",
      "exp = 2.08828\n",
      "Iteration 257\n",
      "loss = 25.7668\n",
      "exp = 2.08666\n",
      "Iteration 258\n",
      "loss = 25.245\n",
      "exp = 2.08505\n",
      "Iteration 259\n",
      "loss = 24.7273\n",
      "exp = 2.08345\n",
      "Iteration 260\n",
      "loss = 24.2135\n",
      "exp = 2.08185\n",
      "Iteration 261\n",
      "loss = 23.7035\n",
      "exp = 2.08026\n",
      "Iteration 262\n",
      "loss = 23.1975\n",
      "exp = 2.07867\n",
      "Iteration 263\n",
      "loss = 22.6952\n",
      "exp = 2.07709\n",
      "Iteration 264\n",
      "loss = 22.1965\n",
      "exp = 2.07552\n",
      "Iteration 265\n",
      "loss = 21.7016\n",
      "exp = 2.07395\n",
      "Iteration 266\n",
      "loss = 21.2104\n",
      "exp = 2.07238\n",
      "Iteration 267\n",
      "loss = 20.7228\n",
      "exp = 2.07083\n",
      "Iteration 268\n",
      "loss = 20.2388\n",
      "exp = 2.06928\n",
      "Iteration 269\n",
      "loss = 19.7583\n",
      "exp = 2.06773\n",
      "Iteration 270\n",
      "loss = 19.2813\n",
      "exp = 2.06619\n",
      "Iteration 271\n",
      "loss = 18.8078\n",
      "exp = 2.06466\n",
      "Iteration 272\n",
      "loss = 18.3377\n",
      "exp = 2.06313\n",
      "Iteration 273\n",
      "loss = 17.871\n",
      "exp = 2.0616\n",
      "Iteration 274\n",
      "loss = 17.4076\n",
      "exp = 2.06008\n",
      "Iteration 275\n",
      "loss = 16.9476\n",
      "exp = 2.05857\n",
      "Iteration 276\n",
      "loss = 16.4908\n",
      "exp = 2.05706\n",
      "Iteration 277\n",
      "loss = 16.0373\n",
      "exp = 2.05556\n",
      "Iteration 278\n",
      "loss = 15.587\n",
      "exp = 2.05407\n",
      "Iteration 279\n",
      "loss = 15.1398\n",
      "exp = 2.05258\n",
      "Iteration 280\n",
      "loss = 14.6958\n",
      "exp = 2.05109\n",
      "Iteration 281\n",
      "loss = 14.2549\n",
      "exp = 2.04961\n",
      "Iteration 282\n",
      "loss = 13.8171\n",
      "exp = 2.04813\n",
      "Iteration 283\n",
      "loss = 13.3824\n",
      "exp = 2.04666\n",
      "Iteration 284\n",
      "loss = 12.9506\n",
      "exp = 2.0452\n",
      "Iteration 285\n",
      "loss = 12.5218\n",
      "exp = 2.04374\n",
      "Iteration 286\n",
      "loss = 12.096\n",
      "exp = 2.04228\n",
      "Iteration 287\n",
      "loss = 11.6731\n",
      "exp = 2.04083\n",
      "Iteration 288\n",
      "loss = 11.2531\n",
      "exp = 2.03939\n",
      "Iteration 289\n",
      "loss = 10.836\n",
      "exp = 2.03794\n",
      "Iteration 290\n",
      "loss = 10.4217\n",
      "exp = 2.03651\n",
      "Iteration 291\n",
      "loss = 10.0101\n",
      "exp = 2.03508\n",
      "Iteration 292\n",
      "loss = 9.60137\n",
      "exp = 2.03365\n",
      "Iteration 293\n",
      "loss = 9.19538\n",
      "exp = 2.03223\n",
      "Iteration 294\n",
      "loss = 8.79209\n",
      "exp = 2.03081\n",
      "Iteration 295\n",
      "loss = 8.3915\n",
      "exp = 2.0294\n",
      "Iteration 296\n",
      "loss = 7.9936\n",
      "exp = 2.028\n",
      "Iteration 297\n",
      "loss = 7.59835\n",
      "exp = 2.02659\n",
      "Iteration 298\n",
      "loss = 7.20567\n",
      "exp = 2.0252\n",
      "Iteration 299\n",
      "loss = 6.81562\n",
      "exp = 2.0238\n",
      "Iteration 300\n",
      "loss = 6.42812\n",
      "exp = 2.02241\n",
      "Iteration 301\n",
      "loss = 6.04314\n",
      "exp = 2.02103\n",
      "Iteration 302\n",
      "loss = 5.66066\n",
      "exp = 2.01965\n",
      "Iteration 303\n",
      "loss = 5.28068\n",
      "exp = 2.01828\n",
      "Iteration 304\n",
      "loss = 4.90318\n",
      "exp = 2.01691\n",
      "Iteration 305\n",
      "loss = 4.52806\n",
      "exp = 2.01554\n",
      "Iteration 306\n",
      "loss = 4.1554\n",
      "exp = 2.01418\n",
      "Iteration 307\n",
      "loss = 3.7851\n",
      "exp = 2.01282\n",
      "Iteration 308\n",
      "loss = 3.41721\n",
      "exp = 2.01147\n",
      "Iteration 309\n",
      "loss = 3.05165\n",
      "exp = 2.01012\n",
      "Iteration 310\n",
      "loss = 2.68842\n",
      "exp = 2.00877\n",
      "Iteration 311\n",
      "loss = 2.32748\n",
      "exp = 2.00743\n",
      "Iteration 312\n",
      "loss = 1.96884\n",
      "exp = 2.0061\n",
      "Iteration 313\n",
      "loss = 1.61247\n",
      "exp = 2.00477\n",
      "Iteration 314\n",
      "loss = 1.2583\n",
      "exp = 2.00344\n",
      "Iteration 315\n",
      "loss = 0.906373\n",
      "exp = 2.00211\n",
      "Iteration 316\n",
      "loss = 0.556617\n",
      "exp = 2.0008\n",
      "Iteration 317\n",
      "loss = 0.209018\n",
      "exp = 1.99948\n",
      "Iteration 318\n",
      "loss = 0.136407\n",
      "exp = 2.00079\n",
      "Iteration 319\n",
      "loss = 0.207952\n",
      "exp = 1.99948\n",
      "Iteration 320\n",
      "loss = 0.137469\n",
      "exp = 2.00079\n",
      "Iteration 321\n",
      "loss = 0.206887\n",
      "exp = 1.99947\n",
      "Iteration 322\n",
      "loss = 0.138533\n",
      "exp = 2.00078\n",
      "Iteration 323\n",
      "loss = 0.20582\n",
      "exp = 1.99947\n",
      "Iteration 324\n",
      "loss = 0.139564\n",
      "exp = 2.00078\n",
      "Iteration 325\n",
      "loss = 0.204754\n",
      "exp = 1.99946\n",
      "Iteration 326\n",
      "loss = 0.140627\n",
      "exp = 2.00078\n",
      "Iteration 327\n",
      "loss = 0.203688\n",
      "exp = 1.99946\n",
      "Iteration 328\n",
      "loss = 0.141689\n",
      "exp = 2.00077\n",
      "Iteration 329\n",
      "loss = 0.202622\n",
      "exp = 1.99946\n",
      "Iteration 330\n",
      "loss = 0.142752\n",
      "exp = 2.00077\n",
      "Iteration 331\n",
      "loss = 0.201556\n",
      "exp = 1.99945\n",
      "Iteration 332\n",
      "loss = 0.143815\n",
      "exp = 2.00076\n",
      "Iteration 333\n",
      "loss = 0.20049\n",
      "exp = 1.99945\n",
      "Iteration 334\n",
      "loss = 0.144878\n",
      "exp = 2.00076\n",
      "Iteration 335\n",
      "loss = 0.199424\n",
      "exp = 1.99944\n",
      "Iteration 336\n",
      "loss = 0.145941\n",
      "exp = 2.00075\n",
      "Iteration 337\n",
      "loss = 0.198358\n",
      "exp = 1.99944\n",
      "Iteration 338\n",
      "loss = 0.147003\n",
      "exp = 2.00075\n",
      "Iteration 339\n",
      "loss = 0.197291\n",
      "exp = 1.99944\n",
      "Iteration 340\n",
      "loss = 0.148066\n",
      "exp = 2.00075\n",
      "Iteration 341\n",
      "loss = 0.196225\n",
      "exp = 1.99943\n",
      "Iteration 342\n",
      "loss = 0.149097\n",
      "exp = 2.00074\n",
      "Iteration 343\n",
      "loss = 0.195159\n",
      "exp = 1.99943\n",
      "Iteration 344\n",
      "loss = 0.15016\n",
      "exp = 2.00074\n",
      "Iteration 345\n",
      "loss = 0.194094\n",
      "exp = 1.99942\n",
      "Iteration 346\n",
      "loss = 0.151223\n",
      "exp = 2.00073\n",
      "Iteration 347\n",
      "loss = 0.193028\n",
      "exp = 1.99942\n",
      "Iteration 348\n",
      "loss = 0.152285\n",
      "exp = 2.00073\n",
      "Iteration 349\n",
      "loss = 0.191962\n",
      "exp = 1.99942\n",
      "Iteration 350\n",
      "loss = 0.153348\n",
      "exp = 2.00073\n",
      "Iteration 351\n",
      "loss = 0.190896\n",
      "exp = 1.99941\n",
      "Iteration 352\n",
      "loss = 0.154411\n",
      "exp = 2.00072\n",
      "Iteration 353\n",
      "loss = 0.18983\n",
      "exp = 1.99941\n",
      "Iteration 354\n",
      "loss = 0.155474\n",
      "exp = 2.00072\n",
      "Iteration 355\n",
      "loss = 0.188764\n",
      "exp = 1.9994\n",
      "Iteration 356\n",
      "loss = 0.156537\n",
      "exp = 2.00071\n",
      "Iteration 357\n",
      "loss = 0.187698\n",
      "exp = 1.9994\n",
      "Iteration 358\n",
      "loss = 0.157599\n",
      "exp = 2.00071\n",
      "Iteration 359\n",
      "loss = 0.186632\n",
      "exp = 1.9994\n",
      "Iteration 360\n",
      "loss = 0.158662\n",
      "exp = 2.00071\n",
      "Iteration 361\n",
      "loss = 0.185566\n",
      "exp = 1.99939\n",
      "Iteration 362\n",
      "loss = 0.159693\n",
      "exp = 2.0007\n",
      "Iteration 363\n",
      "loss = 0.1845\n",
      "exp = 1.99939\n",
      "Iteration 364\n",
      "loss = 0.160756\n",
      "exp = 2.0007\n",
      "Iteration 365\n",
      "loss = 0.183434\n",
      "exp = 1.99938\n",
      "Iteration 366\n",
      "loss = 0.161818\n",
      "exp = 2.00069\n",
      "Iteration 367\n",
      "loss = 0.182368\n",
      "exp = 1.99938\n",
      "Iteration 368\n",
      "loss = 0.162881\n",
      "exp = 2.00069\n",
      "Iteration 369\n",
      "loss = 0.181302\n",
      "exp = 1.99938\n",
      "Iteration 370\n",
      "loss = 0.163943\n",
      "exp = 2.00069\n",
      "Iteration 371\n",
      "loss = 0.180236\n",
      "exp = 1.99937\n",
      "Iteration 372\n",
      "loss = 0.165006\n",
      "exp = 2.00068\n",
      "Iteration 373\n",
      "loss = 0.179171\n",
      "exp = 1.99937\n",
      "Iteration 374\n",
      "loss = 0.166068\n",
      "exp = 2.00068\n",
      "Iteration 375\n",
      "loss = 0.178105\n",
      "exp = 1.99936\n",
      "Iteration 376\n",
      "loss = 0.167131\n",
      "exp = 2.00067\n",
      "Iteration 377\n",
      "loss = 0.177039\n",
      "exp = 1.99936\n",
      "Iteration 378\n",
      "loss = 0.168193\n",
      "exp = 2.00067\n",
      "Iteration 379\n",
      "loss = 0.175973\n",
      "exp = 1.99935\n",
      "Iteration 380\n",
      "loss = 0.169225\n",
      "exp = 2.00067\n",
      "Iteration 381\n",
      "loss = 0.174907\n",
      "exp = 1.99935\n",
      "Iteration 382\n",
      "loss = 0.170287\n",
      "exp = 2.00066\n",
      "Iteration 383\n",
      "loss = 0.173842\n",
      "exp = 1.99935\n",
      "Iteration 384\n",
      "loss = 0.17135\n",
      "exp = 2.00066\n",
      "Iteration 385\n",
      "loss = 0.172775\n",
      "exp = 1.99934\n",
      "Iteration 386\n",
      "loss = 0.172412\n",
      "exp = 2.00065\n",
      "Iteration 387\n",
      "loss = 0.17171\n",
      "exp = 1.99934\n",
      "Iteration 388\n",
      "loss = 0.173475\n",
      "exp = 2.00065\n",
      "Iteration 389\n",
      "loss = 0.170644\n",
      "exp = 1.99933\n",
      "Iteration 390\n",
      "loss = 0.174537\n",
      "exp = 2.00065\n",
      "Iteration 391\n",
      "loss = 0.169579\n",
      "exp = 1.99933\n",
      "Iteration 392\n",
      "loss = 0.1756\n",
      "exp = 2.00064\n",
      "Iteration 393\n",
      "loss = 0.168513\n",
      "exp = 1.99933\n",
      "Iteration 394\n",
      "loss = 0.176662\n",
      "exp = 2.00064\n",
      "Iteration 395\n",
      "loss = 0.167447\n",
      "exp = 1.99932\n",
      "Iteration 396\n",
      "loss = 0.177725\n",
      "exp = 2.00063\n",
      "Iteration 397\n",
      "loss = 0.166381\n",
      "exp = 1.99932\n",
      "Iteration 398\n",
      "loss = 0.178787\n",
      "exp = 2.00063\n",
      "Iteration 399\n",
      "loss = 0.165316\n",
      "exp = 1.99931\n",
      "Iteration 400\n",
      "loss = 0.179819\n",
      "exp = 2.00063\n",
      "Iteration 401\n",
      "loss = 0.16425\n",
      "exp = 1.99931\n",
      "Iteration 402\n",
      "loss = 0.180881\n",
      "exp = 2.00062\n",
      "Iteration 403\n",
      "loss = 0.163184\n",
      "exp = 1.99931\n",
      "Iteration 404\n",
      "loss = 0.181943\n",
      "exp = 2.00062\n",
      "Iteration 405\n",
      "loss = 0.162118\n",
      "exp = 1.9993\n",
      "Iteration 406\n",
      "loss = 0.183006\n",
      "exp = 2.00061\n",
      "Iteration 407\n",
      "loss = 0.161053\n",
      "exp = 1.9993\n",
      "Iteration 408\n",
      "loss = 0.184068\n",
      "exp = 2.00061\n",
      "Iteration 409\n",
      "loss = 0.159987\n",
      "exp = 1.99929\n",
      "Iteration 410\n",
      "loss = 0.185131\n",
      "exp = 2.0006\n",
      "Iteration 411\n",
      "loss = 0.158922\n",
      "exp = 1.99929\n",
      "Iteration 412\n",
      "loss = 0.186193\n",
      "exp = 2.0006\n",
      "Iteration 413\n",
      "loss = 0.157855\n",
      "exp = 1.99929\n",
      "Iteration 414\n",
      "loss = 0.187255\n",
      "exp = 2.0006\n",
      "Iteration 415\n",
      "loss = 0.15679\n",
      "exp = 1.99928\n",
      "Iteration 416\n",
      "loss = 0.188317\n",
      "exp = 2.00059\n",
      "Iteration 417\n",
      "loss = 0.155724\n",
      "exp = 1.99928\n",
      "Iteration 418\n",
      "loss = 0.189348\n",
      "exp = 2.00059\n",
      "Iteration 419\n",
      "loss = 0.154658\n",
      "exp = 1.99927\n",
      "Iteration 420\n",
      "loss = 0.190411\n",
      "exp = 2.00058\n",
      "Iteration 421\n",
      "loss = 0.153593\n",
      "exp = 1.99927\n",
      "Iteration 422\n",
      "loss = 0.191473\n",
      "exp = 2.00058\n",
      "Iteration 423\n",
      "loss = 0.152527\n",
      "exp = 1.99927\n",
      "Iteration 424\n",
      "loss = 0.192535\n",
      "exp = 2.00058\n",
      "Iteration 425\n",
      "loss = 0.151462\n",
      "exp = 1.99926\n",
      "Iteration 426\n",
      "loss = 0.193598\n",
      "exp = 2.00057\n",
      "Iteration 427\n",
      "loss = 0.150396\n",
      "exp = 1.99926\n",
      "Iteration 428\n",
      "loss = 0.19466\n",
      "exp = 2.00057\n",
      "Iteration 429\n",
      "loss = 0.149331\n",
      "exp = 1.99925\n",
      "Iteration 430\n",
      "loss = 0.195722\n",
      "exp = 2.00056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 431\n",
      "loss = 0.148265\n",
      "exp = 1.99925\n",
      "Iteration 432\n",
      "loss = 0.196784\n",
      "exp = 2.00056\n",
      "Iteration 433\n",
      "loss = 0.1472\n",
      "exp = 1.99925\n",
      "Iteration 434\n",
      "loss = 0.197847\n",
      "exp = 2.00056\n",
      "Iteration 435\n",
      "loss = 0.146134\n",
      "exp = 1.99924\n",
      "Iteration 436\n",
      "loss = 0.198909\n",
      "exp = 2.00055\n",
      "Iteration 437\n",
      "loss = 0.145068\n",
      "exp = 1.99924\n",
      "Iteration 438\n",
      "loss = 0.19994\n",
      "exp = 2.00055\n",
      "Iteration 439\n",
      "loss = 0.144003\n",
      "exp = 1.99923\n",
      "Iteration 440\n",
      "loss = 0.201002\n",
      "exp = 2.00054\n",
      "Iteration 441\n",
      "loss = 0.142938\n",
      "exp = 1.99923\n",
      "Iteration 442\n",
      "loss = 0.202064\n",
      "exp = 2.00054\n",
      "Iteration 443\n",
      "loss = 0.141872\n",
      "exp = 1.99923\n",
      "Iteration 444\n",
      "loss = 0.203126\n",
      "exp = 2.00054\n",
      "Iteration 445\n",
      "loss = 0.140807\n",
      "exp = 1.99922\n",
      "Iteration 446\n",
      "loss = 0.204189\n",
      "exp = 2.00053\n",
      "Iteration 447\n",
      "loss = 0.139741\n",
      "exp = 1.99922\n",
      "Iteration 448\n",
      "loss = 0.205251\n",
      "exp = 2.00053\n",
      "Iteration 449\n",
      "loss = 0.138675\n",
      "exp = 1.99921\n",
      "Iteration 450\n",
      "loss = 0.206313\n",
      "exp = 2.00052\n",
      "Iteration 451\n",
      "loss = 0.13761\n",
      "exp = 1.99921\n",
      "Iteration 452\n",
      "loss = 0.207375\n",
      "exp = 2.00052\n",
      "Iteration 453\n",
      "loss = 0.136544\n",
      "exp = 1.99921\n",
      "Iteration 454\n",
      "loss = 0.208437\n",
      "exp = 2.00052\n",
      "Iteration 455\n",
      "loss = 0.135479\n",
      "exp = 1.9992\n",
      "Iteration 456\n",
      "loss = 0.209468\n",
      "exp = 2.00051\n",
      "Iteration 457\n",
      "loss = 0.134414\n",
      "exp = 1.9992\n",
      "Iteration 458\n",
      "loss = 0.210531\n",
      "exp = 2.00051\n",
      "Iteration 459\n",
      "loss = 0.133349\n",
      "exp = 1.99919\n",
      "Iteration 460\n",
      "loss = 0.211592\n",
      "exp = 2.0005\n",
      "Iteration 461\n",
      "loss = 0.132283\n",
      "exp = 1.99919\n",
      "Iteration 462\n",
      "loss = 0.212654\n",
      "exp = 2.0005\n",
      "Iteration 463\n",
      "loss = 0.131217\n",
      "exp = 1.99919\n",
      "Iteration 464\n",
      "loss = 0.213717\n",
      "exp = 2.0005\n",
      "Iteration 465\n",
      "loss = 0.130152\n",
      "exp = 1.99918\n",
      "Iteration 466\n",
      "loss = 0.214779\n",
      "exp = 2.00049\n",
      "Iteration 467\n",
      "loss = 0.129087\n",
      "exp = 1.99918\n",
      "Iteration 468\n",
      "loss = 0.215841\n",
      "exp = 2.00049\n",
      "Iteration 469\n",
      "loss = 0.128021\n",
      "exp = 1.99917\n",
      "Iteration 470\n",
      "loss = 0.216903\n",
      "exp = 2.00048\n",
      "Iteration 471\n",
      "loss = 0.126956\n",
      "exp = 1.99917\n",
      "Iteration 472\n",
      "loss = 0.217965\n",
      "exp = 2.00048\n",
      "Iteration 473\n",
      "loss = 0.125891\n",
      "exp = 1.99916\n",
      "Iteration 474\n",
      "loss = 0.219027\n",
      "exp = 2.00048\n",
      "Iteration 475\n",
      "loss = 0.124826\n",
      "exp = 1.99916\n",
      "Iteration 476\n",
      "loss = 0.220058\n",
      "exp = 2.00047\n",
      "Iteration 477\n",
      "loss = 0.12376\n",
      "exp = 1.99916\n",
      "Iteration 478\n",
      "loss = 0.22112\n",
      "exp = 2.00047\n",
      "Iteration 479\n",
      "loss = 0.122695\n",
      "exp = 1.99915\n",
      "Iteration 480\n",
      "loss = 0.222182\n",
      "exp = 2.00046\n",
      "Iteration 481\n",
      "loss = 0.121629\n",
      "exp = 1.99915\n",
      "Iteration 482\n",
      "loss = 0.223244\n",
      "exp = 2.00046\n",
      "Iteration 483\n",
      "loss = 0.120564\n",
      "exp = 1.99914\n",
      "Iteration 484\n",
      "loss = 0.224306\n",
      "exp = 2.00045\n",
      "Iteration 485\n",
      "loss = 0.119499\n",
      "exp = 1.99914\n",
      "Iteration 486\n",
      "loss = 0.225368\n",
      "exp = 2.00045\n",
      "Iteration 487\n",
      "loss = 0.118434\n",
      "exp = 1.99914\n",
      "Iteration 488\n",
      "loss = 0.22643\n",
      "exp = 2.00045\n",
      "Iteration 489\n",
      "loss = 0.117368\n",
      "exp = 1.99913\n",
      "Iteration 490\n",
      "loss = 0.227492\n",
      "exp = 2.00044\n",
      "Iteration 491\n",
      "loss = 0.116303\n",
      "exp = 1.99913\n",
      "Iteration 492\n",
      "loss = 0.228554\n",
      "exp = 2.00044\n",
      "Iteration 493\n",
      "loss = 0.115238\n",
      "exp = 1.99912\n",
      "Iteration 494\n",
      "loss = 0.229585\n",
      "exp = 2.00043\n",
      "Iteration 495\n",
      "loss = 0.114173\n",
      "exp = 1.99912\n",
      "Iteration 496\n",
      "loss = 0.230646\n",
      "exp = 2.00043\n",
      "Iteration 497\n",
      "loss = 0.113107\n",
      "exp = 1.99912\n",
      "Iteration 498\n",
      "loss = 0.231708\n",
      "exp = 2.00043\n",
      "Iteration 499\n",
      "loss = 0.112042\n",
      "exp = 1.99911\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def rmse(y, y_hat):\n",
    "    \"\"\"Compute root mean squared error\"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square((y - y_hat))))\n",
    "\n",
    "def forward(x, e):\n",
    "    \"\"\"Forward pass for our function\"\"\"\n",
    "    # tensorflow has automatic broadcasting \n",
    "    # so we do not need to reshape e manually\n",
    "    return tf.pow(x, e) \n",
    "\n",
    "n = 100 # number of examples\n",
    "learning_rate = 5e-6\n",
    "\n",
    "# Placeholders for data\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "# Model parameters\n",
    "exp = tf.constant(2.0)\n",
    "exp_hat = tf.Variable(4.0, name='exp_hat')\n",
    "\n",
    "# Model definition\n",
    "y_hat = forward(x, exp_hat)\n",
    "\n",
    "# Optimizer\n",
    "loss = rmse(y, y_hat)\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "\n",
    "# Summaries (NEW)\n",
    "loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "exp_summary = tf.summary.scalar(\"exp\", exp_hat)\n",
    "all_summaries = tf.summary.merge_all()\n",
    "\n",
    "# We will run this operation to perform a single training step,\n",
    "# e.g. opt.step() in Pytorch.\n",
    "# Execution of this operation will also update model parameters\n",
    "train_op = opt.minimize(loss) \n",
    "\n",
    "# Let's generate some training data\n",
    "x_train = np.random.rand(n) + 10\n",
    "y_train = x_train ** 2\n",
    "\n",
    "loss_history = []\n",
    "exp_history = []\n",
    "\n",
    "# First, we need to create a Tensorflow session object\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all defined variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    \n",
    "    summary_writer = tf.summary.FileWriter('./tensorboard', sess.graph)\n",
    "    \n",
    "    # Training loop\n",
    "    for i in range(0, 500):\n",
    "        print(\"Iteration %d\" % i)\n",
    "        # Run a single trainig step\n",
    "        summaries, curr_loss, curr_exp, _ = sess.run([all_summaries, loss, exp_hat, train_op], feed_dict={x: x_train, y: y_train})\n",
    "        \n",
    "        print(\"loss = %s\" % curr_loss)\n",
    "        print(\"exp = %s\" % curr_exp)\n",
    "        \n",
    "        # Do some recordings for plots\n",
    "        loss_history.append(curr_loss)\n",
    "        exp_history.append(curr_exp)\n",
    "        \n",
    "        summary_writer.add_summary(summaries, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# download and transform train dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=True,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# download and transform test dataset\n",
    "train_loader = torch.utils.data.DataLoader(datasets.MNIST('../mnist_data', \n",
    "                                                          download=True, \n",
    "                                                          train=False,\n",
    "                                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
    "                                                          ])), \n",
    "                                           batch_size=10, \n",
    "                                           shuffle=True)\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.dropout = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input is 28x28x1\n",
    "        # conv1(kernel=5, filters=10) 28x28x10 -> 24x24x10\n",
    "        # max_pool(kernel=2) 24x24x10 -> 12x12x10\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        \n",
    "        # conv2(kernel=5, filters=20) 12x12x20 -> 8x8x20\n",
    "        # max_pool(kernel=2) 8x8x20 -> 4x4x20\n",
    "        x = F.relu(F.max_pool2d(self.dropout(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten 4x4x20 = 320\n",
    "        x = x.view(-1, 320)\n",
    "        \n",
    "        # 320 -> 50\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        \n",
    "        # 50 -> 10\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # transform to logits\n",
    "        reutrn F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F.max_pool2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
